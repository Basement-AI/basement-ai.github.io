@inproceedings{b79922af105f463197cdb3bac38ac3f5,
title = "Overview of the “Voight-Kampff” Generative AI Authorship Verification Task at PAN and ELOQUENT 2024",
abstract = "The “Voight-Kampff” Generative AI Authorship Verification task aims to determine whether a text was generated by an AI or written by a human. As in its fictional inspiration, the Voight-Kampff task structures AI detection as a builder-breaker challenge: The builders, participants in the PAN lab, submit software to detect AI-written text and the breakers, participants in the ELOQUENT lab, submit AI-written text with the goal of fooling the builders. We formulate the task in a way that is reminiscent of a traditional authorship verification problem, where given a pair of texts, their human or machine authorship is to be inferred. For this first task installment, we further restrict the problem so that each pair is guaranteed to contain one human and one machine text. Hence the task description reads: Given two texts, one authored by a human, one by a machine: pick out the human. In total, we evaluated 43 detection systems (30 participant submissions and 13 baselines), ranging from linear classifiers to perplexity-based zero-shot systems. We tested them on 70 individual test set variants organized in 14 base collections, each designed on different constraints such as short texts, Unicode obfuscations, or language switching. The top systems achieve very high scores, proving themselves not perfect but sufficiently robust across a wide range of specialized testing regimes. Code used for creating the datasets and evaluating the systems, baselines, and data are available on GitHub.",
keywords = "6121 Languages, 113 Computer and information sciences",
author = "Janek Bevendorf and Matti Wiegmann and Karlgren, {Jussi Jerker} and Luise D{\"u}rlich and Evangelia Gogoulou and Aarne Talman and Efstathios Stamatatos and Martin Potthast and Benno Stein",
note = "Publisher Copyright: {\textcopyright} 2024 Copyright for this paper by its authors.; Conference and Labs of the Evaluation Forum, CLEF 2024 ; Conference date: 09-09-2024 Through 12-09-2024",
year = "2024",
language = "English",
series = "CEUR Workshop Proceedings",
publisher = "CEUR-WS.org",
pages = "2486--2506",
editor = "Guglielmo Faggioli and Ferro, {Nicola } and Galu{\v s}{\v c}{\'a}kov{\'a}, {Petra } and {Garc{\'i}a Seco de Herrera }, {Alba }",
booktitle = "Working Notes of the Conference and Labs of the Evaluation Forum (CLEF 2024)",
address = "Germany",
}